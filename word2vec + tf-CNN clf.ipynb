{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_models.sent2vec import Sent2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_pickle(\"resources/pickle/training.pkl\")\n",
    "testing_data = pd.read_pickle(\"resources/pickle/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "L2_LAMBDA = 10\n",
    "KEEP_PROB = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sent,vec_size):\n",
    "    while len(sent)<20:\n",
    "        sent.append(np.zeros(vec_size))\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(sent_df):\n",
    "    \n",
    "    try:\n",
    "        all_sents = sent_df[\"Sentence\"].values.tolist()\n",
    "    except:\n",
    "        all_sents = sent_df[\"sentences\"].values.tolist()\n",
    "\n",
    "    try:\n",
    "        all_labels = sent_df[\"Label\"].values.tolist()\n",
    "    except:\n",
    "        all_labels = sent_df[\"label\"].values.tolist()\n",
    "\n",
    "    all_labels = np.array([[1,0,0] if x == \"positive\" else ([0,1,0] if x == \"negative\" else [0,0,1]) for x in all_labels])\n",
    "\n",
    "    sent2vec = Sent2Vec()\n",
    "    all_sents = [sent2vec.transform_text_to_vec_matrix(x) for x in all_sents]\n",
    "    vec_size = len(all_sents[0][0])\n",
    "    \n",
    "    padded_sents = [padding(sent,vec_size) if len(sent) < 20\n",
    "                   else sent[:20] for sent in all_sents]\n",
    "    # for sent in all_sents:\n",
    "    #     if len(sent) < 20:\n",
    "    #         sent = padding(sent)\n",
    "    #     if len(sent) > 20:\n",
    "    #         sent = sent[:19]\n",
    "    padded_sents = np.array(padded_sents)\n",
    "    padded_sents = np.expand_dims(padded_sents,axis = -1)\n",
    "    \n",
    "    return padded_sents, all_labels,vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    \n",
    "    if shuffle:\n",
    "        shuffled_index = np.random.permutation(range(x.shape[0]))\n",
    "        x = x[shuffled_index]\n",
    "        y = y[shuffled_index]\n",
    "    \n",
    "    n_batches = int(x.shape[0] / batch_size)\n",
    "    \n",
    "    for i in range(n_batches - 1):\n",
    "        x_batch = x[i*batch_size: (i+1)*batch_size]\n",
    "        y_batch = y[i*batch_size: (i+1)*batch_size]\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sents,all_labels, vec_size= pre_process(training_data)\n",
    "test_padded_sents,test_all_labels,_ = pre_process(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = [2,3,4,5,6]\n",
    "filter_num = 100\n",
    "vec_size = vec_size\n",
    "sent_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/davistanugraha/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/davistanugraha/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-316ca35d8c54>:20: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-316ca35d8c54>:20: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-316ca35d8c54>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-316ca35d8c54>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_input = tf.placeholder(tf.float32,[None,20,50,1])\n",
    "y_input = tf.placeholder(tf.float32,[None,3])\n",
    "pooling_output=[]\n",
    "\n",
    "for i, flt_size in enumerate(filter_size):\n",
    "    flt_shape = [flt_size, vec_size, 1, filter_num]\n",
    "    W = tf.Variable(tf.truncated_normal(flt_shape,stddev = 0.1))\n",
    "    b = tf.Variable(tf.zeros(filter_num))\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_input, W, [1,1,1,1], \"VALID\")\n",
    "    active = tf.nn.relu(tf.nn.bias_add(conv,b))\n",
    "    max_pool = tf.nn.max_pool(active, [1,sent_size-flt_size+1,1,1],\n",
    "                              [1,1,1,1],\"VALID\")\n",
    "    pooling_output.append(max_pool)\n",
    "\n",
    "total_pooling = tf.concat(pooling_output, 3)\n",
    "total_flt = filter_num*len(filter_size)\n",
    "flattern = tf.reshape(total_pooling, (-1, total_flt))\n",
    "\n",
    "dropout = tf.nn.dropout(flattern, KEEP_PROB)\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([total_flt,3],stddev = 0.1))\n",
    "b = tf.Variable(tf.zeros(3)) \n",
    "\n",
    "full_nn = tf.add(tf.matmul(dropout, W), b)\n",
    "pred = tf.nn.softmax(full_nn)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_input, logits = full_nn))\n",
    "loss = loss+L2_LAMBDA*tf.nn.l2_loss(W)\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y_input,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "saver = tf.train.Saver()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "train_accuracy is 0.6015901\n",
      "test_accuracy is 0.47368422 \n",
      "\n",
      "Epoch: 100\n",
      "train_accuracy is 0.73277384\n",
      "test_accuracy is 0.4868421 \n",
      "\n",
      "Epoch: 200\n",
      "train_accuracy is 0.7632509\n",
      "test_accuracy is 0.62171054 \n",
      "\n",
      "Epoch: 300\n",
      "train_accuracy is 0.8701413\n",
      "test_accuracy is 0.68421054 \n",
      "\n",
      "Epoch: 400\n",
      "train_accuracy is 0.8860424\n",
      "test_accuracy is 0.6875 \n",
      "\n",
      "Epoch: 500\n",
      "train_accuracy is 0.8909011\n",
      "test_accuracy is 0.69736844 \n",
      "\n",
      "Model saved in path: saved_cnn_models/cnn_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(501):\n",
    "        for x_train, y_train in get_batch(padded_sents, all_labels):\n",
    "            sess.run(optimizer, feed_dict={x_input:x_train,y_input:y_train})\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x_input:padded_sents,y_input:all_labels})\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x_input:test_padded_sents,y_input:test_all_labels})\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            print(\"Epoch:\", epoch)\n",
    "            print(\"train_accuracy is\", train_accuracy)\n",
    "            print(\"test_accuracy is\", test_accuracy, \"\\n\")\n",
    "    save_path = saver.save(sess, \"saved_cnn_models/cnn_model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
